{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r/relationships Analysis\n",
    "\n",
    "Before the internet, the only peeks into the everyday, non-scripted drama of the strangers around us came through syndicated columns in the newspaper where lovelorn and otherwise distraught souls would write in to Dear Abby or Miss Manners for advice navigating tricky social situations. But times have changed, and like everything else, anonymous advice is available online, 24/7 given by willing strangers. Reddit's [r/relationship forum](https://www.reddit.com/r/relationships/) is probably the premier forum for this, and anonymous users write in by the hundreds every day looking for advice on the harder parts of dealing with other people (and themselves). r/relationships has a bit of notoriety for its [bad advice](http://observer.com/2014/07/this-insane-relationship-advice-subreddit-is-basically-an-online-soap-opera/) and [occasionally bizarre posts](https://twitter.com/redditships?lang=en), but it is also a place where people genuinely do try to help others by dispensing the best advice they can.\n",
    "\n",
    "It also presents an interesting opportunity for big data analysis and Natural Language Processing (NLP) experimentation. This is mostly due to the presence of tags (which I call *keys*) identifying the age and sex of the various people (*actors*) in each social situation the user is writing in about. In this project, I leverage those keys in an algorithm that builds a structured dataset identifying the age, sex and relation to the poster of each person in the situation. These datasets in turn can then be analyzed in interesting ways and produce insightful visualizations, as well as (hopefully) be used to train some machine-learning models in powerful ways.\n",
    "\n",
    "Enough with the introductions, let's get to analyzing some big-time, large-scale drama.\n",
    "\n",
    "\n",
    "\n",
    "## Data Collection and Processing\n",
    "\n",
    "The objective of this step is to build a script that can use logical statements to build structured data out of post titles from r/relationships. The reason I want to build structured data is I want to be able to analyze and train models with this data in a more interesting way than is usually possible with bag-of-words style approaches. Luckily, there is one significant component of these strings that allows for deconstruction of the string: the key. In the title below, the keys are in bold:\n",
    "\n",
    "*Is it wrong of me **[25F]** to hold my brother’s **[32M]** childhood behavior against him?*\n",
    "\n",
    "These keys provide the age and sex of the actors involved, while also providing clues as to relations of those actors to the author (or OP, for original poster). The goal of this script is to use those keys to produce an output like so from the above:\n",
    "\n",
    "| id | age | sex | relation |\n",
    "|----|-----|-----|----------|\n",
    "| 1  | 25  | f   | OP       |\n",
    "| 2  | 32  | m   | brother  |\n",
    "\n",
    "With the script below, I am happy to say I figured out a way of doing that with an error rate of 6% (aka ingested titles that threw an error) and an accuracy rate of about 90% (aka modeled the title incorrectly). \n",
    "\n",
    "----------------------------\n",
    "### Data Collection\n",
    "\n",
    "The data cleaning process is fairly uninteresting but essential so it is included below. I basically use [PRAW](https://praw.readthedocs.io/en/latest/) to scrape post titles, times, IDs, and flair (category) to a SQLite database I store locally. Post text and top comments (when existing) go to text files concurrently. I ran this script once a day or so and picked up ~400 posts on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import datetime\n",
    "from collections import OrderedDict, Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://plot.ly/python/big-data-analytics-with-pandas-and-sqlite/\n",
    "disk_engine  = sqlalchemy.create_engine('sqlite:///relationships.db')\n",
    "\n",
    "# if you are trying to use this yourself you will need to add your own info below\n",
    "r = praw.Reddit(username=\n",
    "                password=\n",
    "                client_id=\n",
    "                client_secret=\n",
    "                user_agent='topPostSnatcher v0.1 by u/reddit2spotify for u/frankhav')\n",
    "\n",
    "submissions = r.subreddit(\"relationships\").new(limit=1000)\n",
    "\n",
    "existing_ids = pd.read_sql_query(\"SELECT id FROM raw_data\", disk_engine).values\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for sub in submissions:\n",
    "    ID= sub.id\n",
    "    flair = sub.link_flair_text\n",
    "    comments_exist = len(list(sub.comments)) > 0\n",
    "    if flair not in ['[new]','Updates'] and sub.id not in existing_ids and comments_exist:\n",
    "        df = df.append(dict(ID= ID,\n",
    "                       title= sub.title,\n",
    "                       date= datetime.datetime.fromtimestamp(sub.created),\n",
    "                       flair= flair\n",
    "    ), ignore_index=True)\n",
    "\n",
    "        with open(\"posts/%s.txt\"%ID, \"w+\") as text_file:\n",
    "            text_file.write(sub.selftext)\n",
    "\n",
    "        with open(\"tcs/%s.txt\"%ID, \"w+\") as text_file:\n",
    "            text_file.write(sub.comments[0].body)\n",
    "\n",
    "df.set_index('ID', inplace=True)\n",
    "df.to_sql('raw_data', disk_engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "Next step is the real meat of the project and involved making some meaningful, structed data out of the titles. To start, I created a large dictionary to generalize the terms used by people when talking about their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relations_dict = {\n",
    "    'OP':['i', 'me', \"i'm\", \"i’m\", \"myself\", 'im'],\n",
    "    'friend':['friend', 'friends'],\n",
    "    'fiance':['fiance', 'fiancé', 'fiancee', 'fiancée'],\n",
    "    'female_SO': ['girlfriend', 'gf'],\n",
    "    'male_SO':  ['boyfriend', 'bf'],\n",
    "    'ambigous_SO':['so','s.o.', 'partner'],\n",
    "    'roommate':['roommate','flatmate'],\n",
    "    'ex':['ex', 'ex-gf', 'ex-girlfriend', 'ex-bf', 'ex-boyfriend', 'ex-bf', 'ex-wife', 'ex-husband'],\n",
    "    'husband':['husband', 'hubby'],\n",
    "    'wife':['wife','wifey'],\n",
    "    'spouse':['spouse'],\n",
    "    'father':['father','dad'],\n",
    "    'mother':['mother','mom', 'mum'],\n",
    "    'parents':['parents'],\n",
    "    'brother':['brother','bro'],\n",
    "    'sister':['sister','sis'],\n",
    "    'ext_fam':['cousin','uncle','aunt','grandfather','grandpa','grandmother','grandma',\n",
    "               'niece','nephew', 'mil', 'fil','sil','bil'],\n",
    "    'neighbor':['neighbor', 'neighbour'],\n",
    "    'colleague':['employee','coworker','boss','manager','supervisor', 'co-worker', 'colleague'],\n",
    "    'fling':['fwb', 'hookup'],\n",
    "    'medical':['doctor','therapist'],\n",
    "    'crush':['crush', 'interest', 'match'],\n",
    "    'other': ['someone', 'person']\n",
    "}\n",
    "\n",
    "weak_relations_dict = {\n",
    "    'other_m':['guy','dude','man', 'he','him'],\n",
    "    'other_f':['girl','chick', 'woman', 'she','her'],\n",
    "    \n",
    "}\n",
    "\n",
    "# lazy change to sets for later\n",
    "for k,v in relations_dict.items():\n",
    "    relations_dict[k] = set(v)\n",
    "    \n",
    "for k,v in weak_relations_dict.items():\n",
    "    weak_relations_dict[k] = set(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t_dissect()\n",
    "\n",
    "Next, I wrote the quite long function below, t_dissect(), which processes the string and surmises the relationships indicated within. The basic approach was to strip the data down of special characters, split it into a list of words in the original order, use a powerful and adaptable regex string to identify keys within the list, then use a series of if/then statements to work through and determine each key's relation to the title.\n",
    "\n",
    "This was an incredibly tricky task that I tried to do with mostly vanilla python and regex. To illustrate some the difficulties, consider the three different ways the following problem could be stated:\n",
    "\n",
    "- My [34/M] husband is constantly criticizing my [30/F] appearance and it is hurting me. What to do?\n",
    "- My [30/F] husband [34/M] is constantly criticizing my  appearance and it is hurting me. What to do?\n",
    "- My husband [34/M] is constantly criticizing my [30/F] appearance and it is hurting me. What to do?\n",
    "\n",
    "The third example is by far the easiest since the two keys are not close in the string. The first two, and especially the second, are syntactically valid but huge headaches. \"My [30/F] husband\" fully indicates that 'my' and 'husband' are referring to 30/F and we only know they are not by the presence of the key following the next key at the 4th phrase in the sentence.\n",
    "\n",
    "One approach would be to try to use the gender of the relation to try to give clues ('son', 'husband' vs 'daughter', 'wife') but there are too many gender-ambigous relations in my relations dictionary for that to be a viable approach. Rather what I do is work backwards through the keys and make a number of assumptions that work pretty well for the most part. The number of if/then trees can be baffling, but it does eventually work out. There are also some lines for a diagnostic mode on the function that just prints out the data at each step of processing, an example of that is in the cell following the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t_dissect(title, diagnose=False):\n",
    "    if diagnose: print(\"Original:\", '\\n', title, '\\n',)\n",
    "    table = str.maketrans({x:' ' for x in list(\"|\\/|()[].?!:,\")})\n",
    "    title = title.translate(table).lower()\n",
    "    \n",
    "    # split the title into a list\n",
    "    title_list = title.split()\n",
    "    if diagnose: print(\"Unprocessed list:  \", '\\n', title_list, '\\n')\n",
    "        \n",
    "    # this obnoxiously long code clears out spaces between the gender and sex markers \n",
    "    # if there is a space between them\n",
    "    age=''\n",
    "    g_index = int()\n",
    "    g_list = [w for w in title_list if len(w) == 1 and w in list('FfMm')]\n",
    "    for word in g_list:\n",
    "            g_index = title_list.index(word)\n",
    "            if g_index == 0:\n",
    "                age = title_list[g_index+1]\n",
    "                title_list[g_index] = word+age\n",
    "                del title_list[g_index+1]\n",
    "            elif g_index == title_list[-1]:\n",
    "                age = title_list[g_index-1]\n",
    "                title_list[g_index] = word+age\n",
    "                del title_list[g_index-1]\n",
    "            else:\n",
    "                age = title_list[g_index-1]\n",
    "                if re.search(r'(\\d+)', title_list[g_index-1]) is None:\n",
    "                    age = title_list[g_index+1]\n",
    "                    title_list[g_index] = word+age\n",
    "                    del title_list[g_index+1]\n",
    "                else:\n",
    "                    title_list[g_index] = word+age\n",
    "                    del title_list[g_index-1]\n",
    "\n",
    "    # removes possessives\n",
    "    for word in title_list:\n",
    "        if word[-2:] == \"’s\" or word[-2:] == \"'s\": title_list[title_list.index(word)] = word[:-2]\n",
    "\n",
    "    if diagnose: print('Processed list:', '\\n', title_list, '\\n\\n-_-_-_-_-_--_-_-_-_-_-\\n')\n",
    "    # find the keys in the title\n",
    "    keys =  []\n",
    "    for word in title_list:\n",
    "        if re.search(r'\\d+(?:,| |\\/|)[sMmFf](?!o)|[sMmsFf](?:,| |\\/|)\\d+', word) is not None:\n",
    "            keys.append(word)\n",
    "    if len(keys) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # set the duplicate flag to true and extract any duplicate keys\n",
    "    duped_keys = []\n",
    "    if len(keys) != len(set(keys)): \n",
    "        duped_keys = [item for item, count in Counter(keys).items() if count > 1]\n",
    "    \n",
    "\n",
    "    # create an ordered list of actors as they appear in the title\n",
    "    p_dict = {x:{'age':None,'sex':None,'relation':None} for x in range(len(keys))}\n",
    "    OP_found = False\n",
    "    n_keys = len(keys)\n",
    "    OP_index = None\n",
    "    if diagnose: print(\"Keys:\", keys, '\\n')\n",
    "        \n",
    "    for n, key in enumerate(keys[::-1]):\n",
    "        # shoot for middle if ambiguity in age by OP (40sM -> 45M)\n",
    "        age_mod = 5 if 's' in key else 0\n",
    "        \n",
    "        # pull the age and sex of the person from the key\n",
    "        p_dict[n]['age'] = int(re.findall(r'(\\d+)', key)[0]) + age_mod\n",
    "        p_dict[n]['sex'] = re.findall(r'([MmFf])', key)[0]\n",
    "        \n",
    "       # get the index of the age/sex key in question and save the two elements on either side of it (if possible)\n",
    "        name_index = title_list.index(key)\n",
    "\n",
    "        before, after = set(), set()\n",
    "        for x in [1,2]:\n",
    "            if (name_index - x) >= 0:\n",
    "                before.add(title_list[name_index-x].lower())\n",
    "            try:\n",
    "                if x == 1:\n",
    "                    after.add(title_list[name_index+x].lower())\n",
    "            except: continue\n",
    "        \n",
    "        def find_relation(before, after, r_dict, OP_found):\n",
    "            if 'my' in before: my_flag = True\n",
    "            else: my_flag = False\n",
    "\n",
    "            # loop through both the before and after words list and identify what categories are identified\n",
    "            before_relation, after_relation = None, None\n",
    "            for k,v in r_dict.items():\n",
    "                if len(v & before) > 0:\n",
    "                    before_relation = k\n",
    "                    continue\n",
    "\n",
    "                if len(v & after) > 0:\n",
    "                    after_relation = k\n",
    "                    continue\n",
    "            \n",
    "            # decide on a relation\n",
    "            relation = 'Unk'\n",
    "            \n",
    "            # if the key is adjacent to \"my\"... \n",
    "            if my_flag:\n",
    "                # and there are no other relations surrounding, assume OP\n",
    "                if before_relation is None and after_relation is None:\n",
    "                    relation, OP_found = 'OP', True              \n",
    "                \n",
    "                # and there is a relation in the before group, assume that relation\n",
    "                elif after_relation is None and before_relation is not None:\n",
    "                    relation = before_relation\n",
    "                    OP_found = True if relation == 'OP' else False\n",
    "                \n",
    "                # and there is a relation in the after group and OP has NOT been found, assume OP\n",
    "                elif before_relation is None and after_relation is not None and OP_found == False:\n",
    "                    relation, OP_found = 'OP', True  \n",
    "                        \n",
    "                # otherwise assume the after relation\n",
    "                elif before_relation is None:\n",
    "                    relation = after_relation\n",
    "                    OP_found = True if relation == 'OP' else False\n",
    "                \n",
    "                # if none of these conditions match ¯\\_(ツ)_/¯ assume before relation \n",
    "                elif OP_found is False:\n",
    "                    relation = before_relation\n",
    "                    OP_found = True if relation == 'OP' else False\n",
    "                    \n",
    "                # unless OP is found, in which case assume after\n",
    "                else:\n",
    "                    relation = after_relation\n",
    "                    if relation == 'OP': OP_found = True\n",
    "                \n",
    "            # if the key is NOT adjacent to \"my\"...         \n",
    "            if not my_flag:\n",
    "                if before_relation is not None: \n",
    "                    relation = before_relation\n",
    "                    OP_found = True if relation == \"OP\" else False\n",
    "                elif after_relation == 'OP': \n",
    "                    relation = before_relation\n",
    "                    OP_found = True if relation == 'OP' else False\n",
    "                else: \n",
    "                    relation = before_relation\n",
    "                    OP_found = True if relation == 'OP' else False\n",
    "            \n",
    "            if n == len(keys) and OP_found is False:\n",
    "                relation, OP_found = 'OP', True\n",
    "            \n",
    "            return relation, OP_found\n",
    "\n",
    "        if diagnose: print(key, \" adjacent: \", before, after)\n",
    "        \n",
    "        # if there is only one key in the title, assume OP. Easy!\n",
    "        if len(keys) == 1:\n",
    "            relation = 'OP'\n",
    "        \n",
    "        # call the above function to find the relation from the primary relations dict\n",
    "        else:\n",
    "            relation, OP_found = find_relation(before, after, relations_dict, OP_found)\n",
    "        \n",
    "        # or the secondary dict if necessary\n",
    "        if relation is None:\n",
    "            relation, OP_found = find_relation(before, after, weak_relations_dict, OP_found)\n",
    "        \n",
    "        # final try: use the gender marker of the key to make a vague guess\n",
    "        if relation is None:\n",
    "            if p_dict[n]['sex'] == 'f': relation = 'other_f'\n",
    "            elif p_dict[n]['sex'] == 'm': relation = 'other_m'\n",
    "            else: relation = 'Unk'\n",
    "            \n",
    "        # if the key is duped, wipe the key just examined from the string so it is not considered in subsequent loops\n",
    "        if key in duped_keys:\n",
    "            del title_list[name_index]\n",
    "            \n",
    "        if relation == 'OP': OP_index = n\n",
    "        p_dict[n]['relation'] = relation\n",
    "    \n",
    "    if diagnose==True: \n",
    "        print('\\n Dictionary result: ')\n",
    "        for k,v in p_dict.items(): print(k,v,)\n",
    "        print('\\n', '\\n')\n",
    "    \n",
    "    if diagnose == False:\n",
    "        return p_dict, OP_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, consider the step-by-step output of this four actor doozy: *\"My [29F] dad [55M] forced my sister [16F] to have a pregnancy test and a urine sample for drugs when her boyfriend [16-17M] dropped her at home last Friday.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      " My [29F] dad [55M] forced my sister [16F] to have a pregnancy test and a urine sample for drugs when her boyfriend [16-17M] dropped her at home last Friday. \n",
      "\n",
      "Unprocessed list:   \n",
      " ['my', '29f', 'dad', '55m', 'forced', 'my', 'sister', '16f', 'to', 'have', 'a', 'pregnancy', 'test', 'and', 'a', 'urine', 'sample', 'for', 'drugs', 'when', 'her', 'boyfriend', '16-17m', 'dropped', 'her', 'at', 'home', 'last', 'friday'] \n",
      "\n",
      "Processed list: \n",
      " ['my', '29f', 'dad', '55m', 'forced', 'my', 'sister', '16f', 'to', 'have', 'a', 'pregnancy', 'test', 'and', 'a', 'urine', 'sample', 'for', 'drugs', 'when', 'her', 'boyfriend', '16-17m', 'dropped', 'her', 'at', 'home', 'last', 'friday'] \n",
      "\n",
      "-_-_-_-_-_--_-_-_-_-_-\n",
      "\n",
      "Keys: ['29f', '55m', '16f', '16-17m'] \n",
      "\n",
      "16-17m  adjacent:  {'her', 'boyfriend'} {'dropped'}\n",
      "16f  adjacent:  {'sister', 'my'} {'to'}\n",
      "55m  adjacent:  {'29f', 'dad'} {'forced'}\n",
      "29f  adjacent:  {'my'} {'dad'}\n",
      "\n",
      " Dictionary result: \n",
      "0 {'age': 16, 'sex': 'm', 'relation': 'male_SO'}\n",
      "1 {'age': 16, 'sex': 'f', 'relation': 'sister'}\n",
      "2 {'age': 55, 'sex': 'm', 'relation': 'father'}\n",
      "3 {'age': 29, 'sex': 'f', 'relation': 'OP'}\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_dissect(\"My [29F] dad [55M] forced my sister [16F] to have a pregnancy test and a urine sample for drugs when her boyfriend [16-17M] dropped her at home last Friday.\", diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aced it! However, you may notice that the 16 year old boyfriend in this title is referenced as the SO (significant other) of the OP, which isn't quite accurate. Second party actors like that are one of the limitations of the current function, along with these other ones I have observed:\n",
    "\n",
    "* **Second degree relations**: As discussed above, this isn't wrong, but it could lead to misinterpretations of data further down the road. Take this title for instance: *I [23F] am concerned that my best friend’s [23F] boyfriend [29M] has a secret son [~4 months]*. All actors get correctly identified, but the \"boyfriend\" in this case is the boyfriend of the best friend, not OP. Nonetheless he gets thrown in the OP relation along with all the other boyfriends. Not a problem really, but should be kept in mind.\n",
    "\n",
    "* **Actor ambiguity**: Take for example this title: *\"Boyfriend [20] broke up with me [18F] yesterday after almost a year. He is everything to me. I feel so lost.\"*. It is fairly obvious the actors involved to us, but since the author tagged \"Boyfriend [20]\", the gender is given by the relation, not the key. Currently one of the big weaknesses of this script is the lack of interaction between the identification of keys and the identifications of the relations, though this would be an easy target for improvement later on.\n",
    "\n",
    "* **regex artifacts**: The regex expression works almost flawlessly with a few exceptions. For example, in *My [26F] boyfriend [27M] of 8M has been working[...]*, 8M (eight months) gets picked up as a key, d'oh!\n",
    "\n",
    "* **Non-binary and trans keys**: I'm not trying to avoid this or anything but it is functionality I have not figured out how to implement yet. Specifically rewriting the regex expression to catch \"MtF\" \"FtM\" etc is a pretty core change, but I would like to get to it soon.\n",
    "\n",
    "---------------------------\n",
    "\n",
    "## Assessing the Script\n",
    "\n",
    "Below, I pull the 2,938 titles I've been scraping from the subreddit for a couple weeks and run t_dissect on them, then assign the results to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_list = []\n",
    "\n",
    "disk_engine  = sqlalchemy.create_engine('sqlite:///relationships.db')\n",
    "titles = pd.read_sql_query(\"SELECT title FROM raw_data\", disk_engine).values\n",
    "\n",
    "test_df = pd.DataFrame(columns=['title', 'n_actors'])\n",
    "for title in titles:\n",
    "    try:\n",
    "        p_dict, OP_index = t_dissect(title[0])\n",
    "        if p_dict:\n",
    "            append_dict = dict(title= title,\n",
    "                               n_actors=len(p_dict),\n",
    "                              OP_age= p_dict.get(OP_index)['age'],\n",
    "                              OP_sex= p_dict.get(OP_index)['sex'])\n",
    "            del p_dict[OP_index]\n",
    "\n",
    "            flat_dict = {}\n",
    "            for x, v in enumerate(p_dict.values()):\n",
    "                new_keys = ['P'+str(x+1)+'_'+y for y in v.keys()]\n",
    "                flat_dict.update(dict(zip(new_keys, v.values())))\n",
    "\n",
    "            append_dict.update(flat_dict)\n",
    "\n",
    "            test_df = test_df.append(append_dict, ignore_index=True)\n",
    "    except:\n",
    "        error_list.append(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles processed:  3142\n",
      "Errors thrown   :  177\n",
      "Error rate      :  5.63\n"
     ]
    }
   ],
   "source": [
    "print(\"Titles processed: \", len(titles))\n",
    "print(\"Errors thrown   : \", len(error_list))\n",
    "print(\"Error rate      : \", round(len(error_list) / len(titles), 4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something upset the script in 177 of the 3142 titles, or just under 6%. Note that errors also occurred when OP could not be identified. Overall, this is pretty good given the range of titles gathered from an \"in-the-wild\" NLP environment. \n",
    "\n",
    "However, error rate is only one question, and not a very important one - the real question is the accuracy of the script. Using very conservative standards for what constitutes \"valid\", I manually validated a random set of 100 of the results in [this Google Sheet](https://docs.google.com/spreadsheets/d/155OywYmF6nHHqjTvLkIiij-OQ2YLw6kMJZjBfCrxztw/edit?usp=sharing) and found an accuracy of **89%** of the titles ingested. \n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = np.random.randint(1, (len(test_df)-500))\n",
    "test_df.iloc[r:r+100].to_csv('Aug24_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One persistent problem I am facing with this script is that it is quite aggressive in assuming author and can sometimes mistake more than one OP despite my best efforts against that. OP ends up being identified for more than one actor, which is obviously incorrect, in 120 of the titles. Since I know this is incorrect, I chose to drop these titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.query(\"P1_relation =='OP' or P2_relation == 'OP' or P3_relation == 'OP'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df.query(\"P1_relation !='OP' and P2_relation != 'OP' and P3_relation != 'OP'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do with this data?\n",
    "\n",
    "Well, what's it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>n_actors</th>\n",
       "      <th>OP_age</th>\n",
       "      <th>OP_sex</th>\n",
       "      <th>P1_age</th>\n",
       "      <th>P1_relation</th>\n",
       "      <th>P1_sex</th>\n",
       "      <th>P2_age</th>\n",
       "      <th>P2_relation</th>\n",
       "      <th>P2_sex</th>\n",
       "      <th>P3_age</th>\n",
       "      <th>P3_relation</th>\n",
       "      <th>P3_sex</th>\n",
       "      <th>P4_age</th>\n",
       "      <th>P4_relation</th>\n",
       "      <th>P4_sex</th>\n",
       "      <th>P5_age</th>\n",
       "      <th>P5_relation</th>\n",
       "      <th>P5_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[25f] my romantic relationship is crumbling b...</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I [19M] think it's time to stop being friends...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>m</td>\n",
       "      <td>18.0</td>\n",
       "      <td>friend</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I am losing myself [23M] in this relationship...</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>m</td>\n",
       "      <td>23.0</td>\n",
       "      <td>ambigous_SO</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[I'm [25/F] going \"on a break\" with my boyfrie...</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>f</td>\n",
       "      <td>23.0</td>\n",
       "      <td>male_SO</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[I [19F] am ridiculously jealous of a friend [...</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>f</td>\n",
       "      <td>21.0</td>\n",
       "      <td>male_SO</td>\n",
       "      <td>m</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title n_actors  OP_age OP_sex  \\\n",
       "0  [[25f] my romantic relationship is crumbling b...        1    25.0      f   \n",
       "1  [I [19M] think it's time to stop being friends...        2    19.0      m   \n",
       "2  [I am losing myself [23M] in this relationship...        2    23.0      m   \n",
       "3  [I'm [25/F] going \"on a break\" with my boyfrie...        2    25.0      f   \n",
       "4  [I [19F] am ridiculously jealous of a friend [...        2    19.0      f   \n",
       "\n",
       "   P1_age  P1_relation P1_sex  P2_age P2_relation P2_sex  P3_age P3_relation  \\\n",
       "0     NaN          NaN    NaN     NaN         NaN    NaN     NaN         NaN   \n",
       "1    18.0       friend      f     NaN         NaN    NaN     NaN         NaN   \n",
       "2    23.0  ambigous_SO      f     NaN         NaN    NaN     NaN         NaN   \n",
       "3    23.0      male_SO      m     NaN         NaN    NaN     NaN         NaN   \n",
       "4    21.0      male_SO      m     NaN         NaN    NaN     NaN         NaN   \n",
       "\n",
       "  P3_sex  P4_age P4_relation P4_sex  P5_age P5_relation P5_sex  \n",
       "0    NaN     NaN         NaN    NaN     NaN         NaN    NaN  \n",
       "1    NaN     NaN         NaN    NaN     NaN         NaN    NaN  \n",
       "2    NaN     NaN         NaN    NaN     NaN         NaN    NaN  \n",
       "3    NaN     NaN         NaN    NaN     NaN         NaN    NaN  \n",
       "4    NaN     NaN         NaN    NaN     NaN         NaN    NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a bit you can do with that. I am planning on building up some cool visualizations and whatnot for the next part of this project, then doing some further bag-of-words and TF-IDF processing to really make things interesting.\n",
    "\n",
    "I will need to develop some tools (or reconfigure my dataframe assignment code) to reflect the quasi-3d nature of the data. By that I mean aside from OP, the other actors in the title are placed according to the syntax of the sentence. As seen below, the majority of titles had only two actors identified, so querying P1 as the primary actor makes sense. In the future I will write functions to handle the dimensionality of the data more completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1885\n",
       "1     343\n",
       "3     194\n",
       "4      20\n",
       "6       3\n",
       "5       1\n",
       "Name: n_actors, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.n_actors.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For now though, the data generated can be queried using standard pandas/SQL methods to answer questions like: Who are females writing about most frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male_SO        398\n",
       "other_m        180\n",
       "friend         130\n",
       "husband         55\n",
       "mother          54\n",
       "ambigous_SO     50\n",
       "ex              47\n",
       "other_f         46\n",
       "colleague       32\n",
       "father          23\n",
       "brother         22\n",
       "fiance          21\n",
       "female_SO       19\n",
       "sister          17\n",
       "roommate        16\n",
       "ext_fam         15\n",
       "parents         11\n",
       "other           10\n",
       "crush            9\n",
       "fling            8\n",
       "neighbor         4\n",
       "wife             2\n",
       "medical          1\n",
       "Name: P1_relation, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.query(\"OP_sex == 'f'\")['P1_relation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is the average age of the girlfriends of 22 year old males?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.68421052631579"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.query(\"OP_sex == 'm' and OP_age == 22 and P1_relation == 'female_SO'\").P1_age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the median age of men writing about mothers vs women writing about mothers? Note I say mothers here rather than their mothers because of the second-degree relation problem mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men:  20.5\n",
      "Women:  23.5\n"
     ]
    }
   ],
   "source": [
    "men = test_df.query(\"OP_sex == 'm' and P1_relation == 'mother'\").OP_age.median()\n",
    "women = test_df.query(\"OP_sex == 'f' and P1_relation == 'mother'\").OP_age.median()\n",
    "\n",
    "print(\"Men: \", men)\n",
    "print(\"Women: \", women)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just a taste of interesting data visualizations to come. For women in their 20s, what is the distribution of age differences between themselves and male SOs mentioned in their posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhCSplwEhSer1/wGeheTR5++gLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b6a97b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = test_df.query(\"OP_age >= 20 and OP_age < 30 and P1_relation == 'male_SO'\")\n",
    "dist = query.P1_age - query.OP_age\n",
    "\n",
    "plt.hist(dist)\n",
    "plt.xlabel(\"Age difference\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Age Differences\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one might notice from everyday observations, women tend to date men older than them and that is reflected in the skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onward!\n",
    "\n",
    "Now that the tough stuff is taken care of, I am excited to do some better visualizations and also test training some models on this data. I'll get around to that soon with another blog post to look out for soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
